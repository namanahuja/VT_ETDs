{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import subprocess\n",
    "import os\n",
    "import xml.etree.ElementTree as ET \n",
    "from xml.etree.ElementTree import Element\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Keywords Searching and Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take single xml document and parse to dictionary\n",
    "def extract_ch_json(docPath, docType,docId):\n",
    "    tree = ET.parse(docPath)\n",
    "    root = tree.getroot()\n",
    "    # initial dict for information storage\n",
    "    outDict = {}\n",
    "    outDict[\"id\"] = docId\n",
    "    outDict[\"type\"] = docType\n",
    "    outDict[\"title\"] = \"\"\n",
    "    outDict[\"chapters\"] = {}\n",
    "    # get document main title\n",
    "    for elem in root.iter('{http://www.tei-c.org/ns/1.0}titleStmt'):\n",
    "        outDict[\"title\"] = elem[0].text\n",
    "    # get chapters\n",
    "    current_chapter = \"\" # current chapter name\n",
    "    find_ch_flag = 0 # find chapter flag skip if not\n",
    "    for div in root.iter(\"{http://www.tei-c.org/ns/1.0}div\"): # iterate all div element\n",
    "        for elem in div.iter(\"{http://www.tei-c.org/ns/1.0}head\"): \n",
    "            if \"chapter\" in elem.text.lower(): # check head element if contains chapter\n",
    "                current_chapter = elem.text\n",
    "                outDict[\"chapters\"][current_chapter] = current_chapter + \"\\n\"\n",
    "                find_ch_flag = 1\n",
    "            elif find_ch_flag:\n",
    "                outDict[\"chapters\"][current_chapter] = outDict[\"chapters\"][current_chapter]+elem.text+\"\\n\"\n",
    "        if find_ch_flag:       \n",
    "            for elem in div.iter(\"{http://www.tei-c.org/ns/1.0}p\"):\n",
    "                outDict[\"chapters\"][current_chapter] = outDict[\"chapters\"][current_chapter]+elem.text+\"\\n\"\n",
    "    if find_ch_flag:\n",
    "        return outDict # return dict if found\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"chapters\": {\n",
      "        \"Chapter 2\": \"Chapter 2\\nLiterature Review\\nMotivating Works\\nWhen approaching background research for this project, it was important to get a good grasp on the types of projects this framework would be supporting. To do this, two main areas were researched. First, a survey of the past two years of CS5604 (Information Retrieval) classes was conducted. The goal was to get a sense of the kinds of work teams are doing with the tweet collections. Second, a review of the IDEAL and GETAR projects was conducted. These two NSF-funded efforts have motivated much of the research work that has gone into creating, maintaining, and researching the digital library of tweets.\\nCS 5604: Information Retrieval\\nCS 5604 at Virginia Tech is a class which uses a project-based learning approach to teach students about information retrieval techniques. The project-based learning approach makes use of a single, semester-long project to guide students through the learning process \\nData I/O\\nEvery team had some kind of data input and output that had to be developed, documented, and coordinated with all of the other teams working on the project. Often, the \\\"Collection Management\\\" teams would be the ones responsible for reading in raw data and processing it into a format that the other teams would be able to work with more easily. The most recent team read the tweet data in the form of raw MySQL databases, and wrote the data to HBase tables for later reference by other teams \\nIn addition to the collection management teams, many other teams had need of various kinds of Input/Output functionality. For example, several teams during the Spring 2016 class needed to read .tsv data files that had been cleaned by the collection management team, alter or analyze them, and then store their results in HBase \\nFrom all this, two main points can be reached. First, teams in large projects like this generally work with other teams by reading data in from a shared source (like an HBase table or a set of source files), processing the data, and then writing their results back to another (or the same, in the case of HBase) shared source. Reading and writing data are widely known to be expensive processes. If the collection of data could be held in a data structure in memory, the process would see a potentially massive increase in efficiency. Second, each team ended up writing their own versions of data reading and data writing code to suit their needs. For each team to spend time writing such similar code seems like a waste of developer hours. If there was a common way provided to them to read and write data in the formats they need, the development process would see more rapid progress towards solving the problem the developers set out to solve, since less time is needed to learn how to access the data in the first place to start experimenting. Both of these points were taken into consideration with many of the design choices made during the development of the framework. They also provide some justification that a system like this would benefit developers working on DLRL research projects.\\nCleaning Tweets\\nThe next topic that emerged frequently was the need to clean the text data. The teams that did the most cleaning were generally the collection management teams \\nThese papers serve to highlight that cleaning is often an essential pre-processing step. It must be performed on raw text data before any analytics are performed, or the results may be inaccurate (if they exist at all). The framework will take this into account by enabling cleaning processes to be done quickly and easily.\\nSpark/Hadoop Learning Curve\\nThe last topic of interest gathered from the project reports was the amount of time teams dedicated strictly to learning Spark and Hadoop (and their associated tools) or reworking code that was written in some other environment to work with the suite of tools on the cluster. There were teams who needed time to understand what tools were available and how to use them to address the problem they were trying to solve. Teams with some prior experience were able to accomplish this in one or two weeks \\nThe amount of time teams spent learning the software tools or working with other systems to avoid having to learn the tools right away indicates that the Spark and Hadoop suites have a very steep learning curve. Developers of varying abilities and backgrounds will handle this learning curve very differently. Some will be able to quickly become good at the tools and use them to their full potential, while some may spend several weeks learning how to use the tools. With this in mind, a priority of the framework is to attempt to smooth out this learning curve without hindering the ability of developers who are more comfortable with the tool suites to still make use of the more advanced features of Spark and Hadoop.\\nSummary\\nThese class projects provide a lot of insight into what developers aiming to do research involving the DLRL's tweet archives may struggle with. The students entering these classes often have little to no background knowledge of the Apache Hadoop and Spark tools, and need to learn them quickly so as to not fall behind in the semester project. Addressing the needs of these students will go a long way towards addressing the needs of DLRL researchers in general. The three main points discussed above all came together to form the key ideas behind the design of the framework.\\nIDEAL and GETAR\\nThe archiving work done in the DLRL is motivated by two NSF-funded projects: the Integrated Digital Event Archiving Library (IDEAL) project \\nTechnical Specifications\\nThis section presents a summary of some of the hardware and software available to developers in the DLRL. It discusses the Hadoop cluster \\nHadoop\\nApache Hadoop is an open source software suite for \\\"reliable, scalable, distributed computing. \\\" \\nSpark\\nApache Spark is \\\"a fast and reliable engine for large scale data processing\\\" \\nUsing Hadoop and Spark\\nThe framework will be designed to work on top of Hadoop and Spark, to apply their functionalities to collections of tweets. Similar to the way Spark abstracts the need for developers to be concerned with parallel programming, the framework will further abstract the need for developers to be concerned with processing raw text data with low-level Spark programs. Instead, the framework will present them with data structures that can be operated upon using provided functionalities, and all interfacing with Spark and Hadoop will be handled for the developer. This will enable developers to operate at a higher level, focusing on working with tweets and collections of tweets rather than raw text.\\n\",\n",
      "        \"Chapter 3\": \"Chapter 3\\nDesign and Architecture\\nDesign Goals\\nMotivating Problems\\nOur framework is designed to support the research and development activities of researchers in the Digital Library Research Laboratory. There are multiple projects happening in the lab at any given time, many of which revolve around our archive of data collected from Twitter through various methods. With so much going on, a tool to help smooth development would save a lot of time in the long run. To assist in development, this framework aims to address two main issues which come up frequently.\\nThe first major issue is that the Apache Spark and Hadoop tool suites have a very steep learning curve. These are the tools that our hardware and software systems support, and so they are used often. However, learning the basic fundamentals can still be a daunting task. It may take students who are just learning to use the tools for the first time several weeks to begin feeling comfortable working with them, and even longer to gain a strong grasp of how to leverage these tools to their full potential. Smoothing out this learning curve would afford researchers more time to focus on the problems they are trying to solve, since they could spend less time learning the tools. This framework solves this issue by abstracting some of the more complicated aspects of working with Spark and Hadoop, and handling them for developers.\\nThe second issue is that there is no well established \\\"best\\\" way to handle reading, writing, and processing of our archived tweet collections. Whenever a new project begins, the first step is frequently to figure out how to retrieve data from the archive, decode it into a workable format, and clean away any unwanted content. There are some tools available to do this including interfacing with Spark and manually transforming the text, using Python scripts and libraries, and using .pig scripts to run on top of Hadoop. However, none of these tools are designed to work directly with our data, and so they all require some other setup or additional programming. This framework is designed with the DLRL's digital Twitter library and the needs of the lab's developers in mind. Hence, there is minimal setup and very little code required to get a collection pulled, formatted, and cleaned.\\nDesign Principles\\nTo address these two problems, the framework is designed with a few guiding principles in mind. These principles were formed over time based on the needs of researchers being supported by the case studies presented in Chapter 5. There are three main design principles.\\nEnable High Level Programming\\nThe first design principle of the framework is to eliminate the need to work with raw text data. This is handled by the data structures provided by the framework, so that developers do not have to interface directly with the software on the cluster to manually parse raw text. Developers should be able to focus on the problems they need to solve with minimal time spent learning the required low level techniques, especially if those low level techniques are addressing problems that have been solved before.\\nEase of Access to Tweet Collections\\nSecondly, the tool should provide developers with quick and convenient access to the collections contained in our archives, as well as a suite of functions to clean and filter the collections. Tweet collections can come from various sources and in various formats. The framework aims to simplify the process of accessing data by making it so that as many of these various sources and formats of data as possible can be processed into one uniform data structure. The currently supported formats and discussion about extending the framework to support new formats of data are documented below in the Tweet Data Structures section of this chapter. In addition, cleaning of data is often a first step in many text mining problems, so performing that first step for developers will accelerate the process of addressing the problem that they are trying to solve.\\nSupport of Tool Development\\nFinally, the framework will support the development of tools to be used by other researchers in the future. A tool in this context refers to anything that can take a tweet collection as an input, process it, and produce some output. Examples might include classification tools, topic modeling techniques, and feature extraction tools. By designing tools to work with these data structures rather than with raw text data, we can make it easier for the tools to support arbitrary collections of tweets, rather than requiring whatever text format the developers happened to be working with when the need arose for a tool to process the data.\\nDesign Overview\\nAs mentioned previously, the main way this framework aims to help developers is by eliminating the need to interface directly with the software on the cluster as much as possible. This is accomplished primarily by the creation of data structures to represent tweets and collections of tweets. By organizing the raw data using such data structures, we are able to represent the text content in a higher-level, easier to work with manner. The two primary data structures are Tweet and TweetCollection. The Tweet data structure contains fields which represent important information about tweet content, as well as functionality to clean the text content and extract important features. The TweetCollection data structure provides a higher level representation of a collection of tweet objects, and provides functionality to clean and filter the collection, etc. By representing tweets and collections of tweets in this way and \\\"black-boxing\\\" the code which reads and deciphers the data files, cleans the individual tweets, and filters the collection, we can speed up development and \\nData Flow and Scope\\nFigure 3.1 shows an overview of how data flows through the framework. The goal is a seamless flow from files stored on disk to a cleaned and processed data structure which can be written back to disk or read in by analytics tools which are designed to take TweetCollections as parameters. Those tools can then use the data structure rather than the raw text to perform analyses. We break down the flow step-by-step here, highlighting important details that go into each step.\\nStep 1: Reading Data from Disk\\nAs mentioned previously, data sets can come from a multitude of places. Some examples include .avro archive files, plain text files, and \\\"separated values\\\" files (.csv or .tsv). If a developer has direct access to the files, they can specify a path to the appropriate TweetCollectionFactory build method to have the file read from disk. TweetCollectionFactory also provides functionality to simply specify a collection ID number as given on the DLRL Hadoop Cluster Page \\nStep 2: Parsing into Tweet Objects\\nOnce the file has been located and opened, the TweetCollectionFactory instructs Spark to read the contents of the file and parallelize its contents across the cluster. This allows for massively parallel (and therefore very fast) reading and processing of the data contained in the file. The tweet data is then remapped from its original format and wrapped in the appropriate Tweet implementation. Each Tweet object is designed to automatically process any metadata it can gather about the tweet based on the data provided. The exact metadata gathered depends on which Tweet implementation is being used. Every implementation is required to extract (or attempt to extract) hashtags, mentions, and URLs; determine whether or not this is a retweet; and break the raw text into tokens. More specific implementations which extend the base SimpleTweet class (explained below in the System Architecture Details section of this chapter) may yield additional metadata based on what is provided by the data it is reading in.\\nStep 3: Cleaning and Filtering the Collection\\nOnce the collection is created and populated with tweet content, developers can manipulate the collection in a variety of ways to clean and filter its contents. There are several built-in functions which can be used to filter the contents of the collection, removing unwanted elements based on certain conditions. Developers can also define custom filter functions which take tweets as parameters and return a Boolean value to tell the collection whether or not to keep that tweet. The framework will handle packaging the function and distributing it across the cluster to be run on every Tweet in the collection.\\nStep 4: Running Analysis Tools on the Data Structures\\nOnce the developers have cleaned the tweets and filtered the collection to their liking, they can proceed to pass the collection on to the various tools. The framework provides several tools, as well as ways to dump the cleaned collection to a file to be passed on to external tools. Tools which are designed to work with the TweetCollection data structure rather than raw text data are able to use its functionality to their advantage. They may do more of their own cleaning or filtering, and/or use TweetCollection's applyFunction functionality to apply some analysis to every tweet contained in the collection. For example, the LDAWrapper (explained in Chapter 4) defines a function which, once topic analysis is completed, takes each tweet, adds a set of topic labels to it, and returns it to the TweetCollection. The topic labels are stored in the Tweet's payload field.\\nScope\\nIt is important here to consider the limits of the scope of the framework as well. In simple terms, as long as tools connected with the framework are doing their work within TweetCollection's provided functionalities, they are within the scope of the framework. That being said, TweetCollection does provide developers with access to the Spark data structures with which it wraps around and interfaces. Some developers may want to access these inner data structures directly rather than via the framework's wrapper functionalities to do a more \\\"low-level\\\" analysis using some more advanced features of Spark. Once this happens, though, they are operating outside of the scope of the framework and are responsible for handling the results on their own. Note, though, that in some cases it is possible to use these data structures to do some analysis and then write the results back to the individual Tweets within the collection. For example, the LDAWrapper tool explained in Chapter 4 does this to give each tweet a set of topic labels while also returning an array of overall topic analysis results. \\nSystem Architecture Details\\nOverview\\nThe system architecture is designed to be modular and extensible. The intention is that developers will be able to extend it to suit their needs and the needs of other researchers doing similar work in the future. Several key design decisions were made with this goal in mind.\\nTweet Data Structures\\nThe fundamental data structure of the whole framework is the Tweet Data structure. Tweet is an abstract data structure; it is not meant to be directly instantiated. There are multiple reasons for this which are discussed in detail in Chapter 4, but the most important reason is that Tweets are constructed very differently based on the format of the data they are being built from. The framework deals with this by representing a tweet in an abstract, high level manner by defining a set of fields in the data structure which all tweets will have in common. It is the responsibility of the concrete implementation of the abstract Tweet class to populate those fields once the data is passed to its constructor. In addition, Tweet requires some basic cleaning functions and utility functions. Most of the functions are abstract, to be implemented by subclasses, but it does provide base functionality for the toString and equals functions. Details about the fields are shown in \\nIt is also worth noting that Tweet requires text content and a string ID as constructor parameters. The text content should represent the text written in the tweet. The ID is a unique identifier to represent this tweet. The ID could be provided by the source of the data (for example, Twitter provides unique IDs for every tweet \\nThere are several concrete implementations of Tweet provided by the framework, which are discussed in detail here. A long term goal for the framework is to make it easy for developers in the future to write new implementations of Tweet to meet their needs if the current set of implementations is not compatible with their data. \\nSimpleTweet\\nSimpleTweet is the basic implementation of Tweet. It provides a concrete implementation of everything required by the abstract class. It can be thought of as the most straightforward and simple (hence the name) representation of a tweet. A SimpleTweet requires only an ID and text content to be constructed. When provided with these, it will automatically mine the text content for hashtags, mentions, and URLs; determine if the tweet is a retweet based on the presence of an RT token; and split the text up into tokens for later processing. It also provides implementations of all of the abstract methods required by Tweet. Since it handles generation of all of the fields and implements all of the abstract methods, SimpleTweet serves as both a rudimentary tweet representation and a base concrete implementation for other, more complex tweet representations (like AvroTweet, SVTweet, and HBaseTweet), discussed next.\\nAvroTweet\\nAvroTweet is an extension of SimpleTweet that extracts content and metadata from .avro files. It takes a single GenericRecord \\nSVTweet\\nSVTweet is another extension of SimpleTweet that builds data structures out of lines in a \\\"Separated Values\\\" (sv) file. These files use a separator character to split lines of data into columns, with each column containing a different part of the data. Common sv files include .csv files (comma separated values) and .tsv files (tab separated values). SVTweet will take these columns and store them in the key/value payload defined in the base Tweet class. By doing this, we can support any amount of arbitrary data that may be stored in the files. This is an important functionality to support, since sv files do not have a defined structure like Avro files do. Some will specify the column names in the first line of the file, but this is not required, and so we cannot depend on it to determine the file schema. Instead, we define a new configuration class called SVConfig which is passed as a parameter to SVTweet.\\nSVConfig\\nSVConfig is a class defined to be passed as a parameter to SVTweet's constructor.\\nIt is used to define several things that are required to be able to parse an sv file, including:\\n\\u2022 The separator value that defines the columns\\n\\u2022 The number of columns contained in the file\\n\\u2022 The column that contains the text of each tweet\\n\\u2022 The column that contains the ID of each tweet\\n\\u2022 The names of any other columns of interest in the file to be stored in the tweet's payload.\\nBy defining these as options, the framework provides the developer with maximum flexibility to parse any kind of sv file containing any kind of data.\\nOne other important thing to note about SVTweet is that the framework cannot currently handle files where the tweet text contains extra newline or separator characters. This would cause the data to become corrupt since the columns would not match. To handle this, the framework will disregard any lines in the file which do not have the correct number of separator characters to match the number of columns specified in SVConfig. The ideal solution would be to combine subsequent lines until the correct number of columns are represented, but due to the way collections are distributed across the cluster by Spark, this would be a very difficult and expensive process. In this case, it is the responsibility of the developer to make sure the data is formatted correctly.\\nHBaseTweet\\nAt the time of this writing, HBaseTweet is still a work in progress. The plan is to have a config parameter similar to SVTweet which specifies what data is contained in which columns. The data structure will automatically handle connecting to HBase and reading data from the table. Thanks to the Spark API, building a collection of these should be a very fast process even though it will require many lookups, since it is all done in parallel.\\nCreating New Tweet Structures\\nExtensibility is a primary goal of this project. Making the creation of new data structures (to meet the needs of future developers) as easy as possible was a high priority when designing the structure of the system. More of the technical information that would be of interest to developers is covered in Chapter 4.\\nThe TweetCollection Data Structure\\nTweetCollection is the second core data structure of the whole system. It is the main structure that will hold all of the tweets parsed out of a collection, and that with which all of the tools designed for this framework will be designed to work. A TweetCollection is just what the name indicates: a collection of Tweet objects. At its most basic, it is a wrapper around Spark's RDD (Resilient Distributed Dataset) data structure \\nTweetCollection offers a suite of ways to clean, filter, and otherwise modify the tweet collection in ways that will be beneficial to the kinds of digital library research that will need this kind of data structure. Filtering functions modify the collection, as opposed to the contents within the collection. Included are ways to remove tweets which meet (or do not meet) specific criteria. Explicit functions filter matches or non-matches based on the contents of a tweet. For example, a researcher may only be interested in tweets within a larger collection that contain a specific hashtag. Maybe the larger event sparked some smaller discussion with the new hashtag and it all was collected together. A developer could very easily load the data into a TweetCollection, filter out all of the tweets which do not contain the hashtag of interest, and pass the new sub-collection along to the various provided tools for analysis or write the results to a file to be processed again later. There are also several utility functions, and room to add more in the future. \\nCreating TweetCollections\\nThe creation of TweetCollections requires four things:\\n1. An ID for the whole collection The first three requirements are fairly straightforward. The ID can be any string. It should provide some high level description of what is in the collection (for example: \\\"HurricaneMatthew\\\" or \\\"#Blacksburg\\\"). The Spark Context \\nThe fourth parameter is the more complicated one. The collection needs to be passed an RDD of Tweet objects. At this point, it is important to address why the system is designed this way. Requiring the RDD to be created external to the data structure affords developers a lot of freedom. More advanced developers may wish to manipulate the RDD using some more advanced Spark functionalities before passing them to the TweetCollection. However, since this framework is primarily aimed at developers who wish to avoid working with Spark, this presents a complication. To remedy this, the framework provides a factory class \\n\\u2022 Creation from .avro files given a path The developer can choose whichever one of the creation functions suits his or her needs, and the entire process of creating an appropriate TweetCollection will be handled automatically. For example, if the developer wants to create a set of collections corresponding to some .avro files stored in the archive, he or she would simply need to instantiate a TweetCollectionFactory with a Spark Context and an SQL Context, and call the createFromAvro function specifying a collection ID (any string) and the collection number found in DLRL's Tweet Archive DB page \\n\",\n",
      "        \"Chapter 4 Implementation\": \"Chapter 4 Implementation\\nData Structure Implementation Details\\nMuch of the effort behind this project was spent refining the way the data structures function. Several radically different iterations were considered until the current generics-based approach was settled upon. Early versions of the project relied too heavily on the TweetCollection data structure, creating separate implementations for every type of data that may have to be read and processed. Each implementation would extend a base collection class that provided some functionality and required all data to be represented as tuples containing the tweet IDs and text content. A later implementation introduced the concept of Tweet data structures to allow for a more complete representation of a tweet, but a large majority of the data processing was still handled by the TweetCollection data structure, and a separate collection class was still necessary for every different kind of data. Finally, it was decided to move the processing of the tweet data into the tweet objects themselves, and handle dealing with different kinds of tweets with Scala Generics \\nInitial Implementation\\nThe original intention was to create multiple different tweet collection structures that would be designed to process various different kinds of data files. At the time, the goal was to be able to handle separated values files and .avro files stored in HDFS, with a general \\\"catch-all\\\" structure to handle RDDs. Within these collections, tweets were represented as (String, Array \\nThe implementation at this point was a very naive one. It was sufficient at the time for initial proof of concept experiments, but quickly fell apart due to a few major flaws. The biggest problem, however, was that representing the tweets with nothing more than an ID and the tokens was extremely restrictive. There was no way to represent more metadata about a tweet without storing it somewhere else or extracting and re-extracting it every time it was needed.\\nThe biggest take-away from this version of the implementation was that polymorphism was, as expected, a powerful way to make the data structures more robust and extensible. The base TweetCollection class was able to provide most of the functionality that the sub-classes would need. In fact, the sub-classes did not need to do anything other than process the input data into the (ID, tokens) tuple. Once that was done, all of the functionality present in TweetCollection would function with no other overriding needed. This was a very flexible approach, and similar polymorphic approaches were used in every subsequent implementation of the framework, including the final release version.\\nIntroduction of the Tweet Class\\nIt quickly became necessary to address the lack of flexibility inherent in using (String, Array \\nOnce DataFrames were ruled out, the next approach was to define a Scala class to represent a tweet object. Doing this would afford the system several key advantages, namely:\\n1. Maximum flexibility for representing tweet metadata, 2. Modular code through defining functions in the tweet classes, and 3. Simplified adding of new data to data structures when tweets implement a \\\"payload\\\".\\nWith this in mind, the framework was refactored to revolve around the newly defined Tweet data structure. The original version of this data structure is shown in \\nWith the introduction of the Tweet data structure, the framework started to look similar to the way it does today. The overall structure of the framework at this point is shown in \\nApplying Generics\\nTo address the issue of redundancy and to simplify development of future data structures, a design was needed that would avoid having a different TweetCollection data structure for each new Tweet implementation. With this in mind, the TweetCollection data structure was redesigned to be a generic data structure \\nSecond, the framework is easy to extend thanks to its highly polymorphic approach. All that is required for a new tweet representation to be accepted by the TweetCollection is that it extend the abstract Tweet class. This can be done by extending the Tweet class directly, providing functionality for each of the abstract methods, and providing content for each of the fields. However, it is even easier to extend the framework by extending the SimpleTweet class. SimpleTweet is the most basic implementation of the Tweet class. It provides implementations for each of the functions and fills all of the necessary fields using only text content and a string ID. AvroTweet, HBaseTweet, and SVTweet all extend SimpleTweet rather than Tweet and leverage its implementations of the functions and fields, while also providing additional data fields and functionality on top of the basics required by Tweet.\\nFinally, the use of generics allows the TweetCollection class much more flexibility. First, it allows the class to make some assumptions about the contents of the collection by requiring that the only types allowed to be stored in the collection be types which have Tweet as a superclass. This allows it to implement several filter methods to make it easier for the developer to prune the collection of unwanted content. Generics also allow TweetCollections to safely implement certain functions which rely on other functions. Since Scala is a functional programming language \\nFramework Tools\\nAs discussed previously, one of the goals of the framework is to support the development of tools which use the data structures to run analyses on collections of tweets. These tools will be able to work with the data without having to interface with Spark and without having to manipulate raw text data. The data structures will provide data for them to process without having to extract it manually. This section details the implementation of three tools and explains how they highlight different capabilities of the framework. The explanations aim to provide some insight for future developers who want to create a tool with this framework.\\nBasic Processing of Collections\\nThe first provided example tool is the WordCounter. This tool was created very early in the framework's development as a proof of concept. This tool was always the first example tool that would be reworked during a new iteration of the framework since it is a very straightforward application of the framework. The tool is designed to take a collection and create a data set of (String, Int) pairs representing each word present in the collection and the number of times each word appears. This is accomplished with a function called count() which takes a TweetCollection as a parameter, and counts all of the words in the collection. This example is simple because it does not use any of the functionalities of the framework beyond taking the TweetCollection data structure as a parameter. This basic utilization of the collection is still powerful because it allows the tool to assume that the collection has already been cleaned and filtered in a way that suits the interests of the developer. \\nCreating and Extracting Metadata\\nThe next included tool is the FeatureExtractor tool. This tool provides functionality to extract specific metadata details about tweets and return them as pairs of tweet IDs and content. For example, the developer might want to gather all of the mentions, hashtags, or URLs from a TweetCollection and hold them in one place for further analysis. Some of the possible outputs are shown in \\nCreating New Metadata\\nThe most complex of the provided tools is the LDAWrapper tool. The intention of the LDAWrapper is to provide an easy way for developers to perform LDA topic analysis \\n1. provide default/suggested parameters for complicated analyses, 2. very easily tweak those parameters to optimize results, and\\n3. read and write metadata to a Tweet object while also returning a set of results.\\nEach of these key points is highlighted by the LDAWrapper in different ways. The first two points are highlighted in \\nCase Studies\\nSince this project was designed with DLRL developers and researchers in mind, several case studies were conducted to test the functionality of the framework, and to gather feedback on its flexibility and ease of use. The case studies cover a wide variety of applications of the framework to justify its design, flexibility, and applications for future work.\\nCase 1: Building a New Tool\\nThe first case study involves a project for the class CS 6604: Digital Libraries. This class allowed students to explore projects of their own design within the domain of digital libraries. In the class, I was on a team with Abigail Bartolome, Radha Krishnan Vinayagam, and Rahul Krishnamurthy. The goal of our project was to build a system which takes a tweet collection and performs sentiment analysis within topics. The system functions by performing user-guided topic analysis on a set of tweets, creating sub-collections of tweets which fall within the same topics, and performing sentiment analysis on those sub-collections. The end result is a set of these sub-collections, which can be used to gather insight into public opinion about these events. The first step in the process is to load the collections and clean them. This process is accomplished quickly and easily with the help of the framework. Since the tool being created for this project is specifically designed to work with the collections archived on the DLRL cluster, it was straightforward to use the TweetCollectionFactory to generate collections of Tweets by simply specifying a collection ID and a collection number. The collection ID was a string describing the contents of the collection (for example \\\"Winter Storm\\\", \\\"Blacksburg\\\", or \\\"Hurricane Matthew\\\"), and the collection number was the number corresponding to that collection on the DLRL Tweet Archive DB web page \\nThe next step was to perform topic analysis, which was also accomplished with the help of the framework. The framework provides the LDAWrapper tool, which is a tool that applies Spark's built in LDA tool \\nThis case study highlights two very important points. First, it highlights the ease of use of tools which are designed to be run on the data structures. Spark's LDA tool requires a fair amount of setup, but having a tool which is designed to take a tweetCollection and use its functionality to handle that setup for you saves much development time as well as lines of code. Making use of the framework tools saved the developers roughly 25 lines of complex Spark code by performing all of the setup required to run Spark's LDA library, and roughly 20 additional lines of similarly complex code by transforming the results of Spark's LDA analysis into a more human-readable and easier to process format. Recall \\nThe second major point demonstrated by this case study is the flexibility that the framework provides to projects like this. For example, at one point during the project's development, the decision was made to switch from implementing the system on the DLRL cluster to implementing on a local Cloudera virtual machine. This meant that testing now needed to be done with local .tsv files rather than the cluster's archive files. Since the project was making use of the framework and not manually deciphering data files, only one line of code needed to be changed to read in the new data. Everything else still worked with no issue even though the files were completely different and provided different data because the analysis was being performed on the data structures rather than raw text data. This saved roughly a day's worth of work that would have been needed to rewrite all of the code which reads in the data, and to rework the analysis code to function with the new format.\\nCase 2: Applying Provided Tools\\nThe second case study is Abigail Bartolome's initial work towards her thesis project, which focuses on linguistic analysis of tweets that discuss the three major hiking trails in the United States: the Appalachian Trail, the Continental Divide Trail, and the Pacific Crest Trail. Abigail is still refining her approach to her thesis work, and so she was interested in seeing what the topic models of her collections would look like. She used the framework's LDAWrapper in a similar manner as the CS 6604 case FIGURE 5.1: Optimized LDA parameters for topic analysis on the hiking trail data sets study. She was able to leverage the tool's flexibility to refine the LDA parameters and generate meaningful topics that would help her focus her research. \\nCase 3: Exploring Data Sets\\nThe third case study explores Islam Harb's work with sets of tweets discussing Hurricane Matthew. This case is particularly interesting for two reasons. First, the Hurricane Matthew data sets are quite large, some of the largest in DLRL's archives \\nIslam was unable to find time to write code himself due to other obligations this semester, but I was able to run the files he provided through the framework tools and give him a set of results. There were four provided data files, ranging in size from just under 153,000 tweets (roughly 55.5 MB) to over 200,000 tweets (roughly 74.5 MB). Each of the files was run through five processes:\\n1. Word Count -Find the most common words mentioned in the collection 2. Hashtag Extraction -Find the hashtags that appear in the collection 3. Mention Extraction -Find the mentions that appear in the collection 4. URL Extraction -Find the URLs that appear in the collection 5. LDA Topic Analysis -Find the topics that are being discussed Each of these analyses was completed, and produced results within a useful amount of time. The extraction processes were the fastest, completing in under a minute each. The topic analysis was the longest, but still completed in only a few minutes thanks to the power of the cluster and the ability to process the collection in a massively parallel manner. In this case study, the suggested parameters provided in the default LDAWrapper were refined to suit our environment. Namely, it was decided to use 100 LDA iterations because with large files like this, going above 100 begins to massively slow the process down. The jump from 100 to 200 slows the process to a few hours rather than a few minutes.\\nThe result files were given back to Islam for review, and he was quite satisfied with the ability of the framework to deliver a variety of results in a short amount of time, and with minimal code needing to be written.\\nCase 4: Building Metadata\\nThe final case study documents Liuqing Li's work towards extracting the URLs contained in tweets and expanding them into full length URLs. Twitter's API automatically shortens every URL that gets posted in a tweet to a \\\"t.co\\\" link \\nLiuqing was given a recent version of the compiled framework to run on the cluster, the source code for the framework itself and all of the currently existing tools, and a brief overview of all of the available functionalities. He was then left to use everything provided to create a new tool which would extract the short URLs, expand them, and fetch the Web content from the Internet Archive. Unfortunately, there were some complications with the implementation due to the fact that the worker nodes on the cluster are not connected to the Internet. This meant that the worker nodes would have no way to fetch content from the Internet Archive. With this in mind, Liuqing decided to instead create a tool which would, for each tweet that contained one or more URLs, print its ID, Timestamp, and the URLs it contains to a file. That file could then be fed into other tools which would gather the Web content and archive it for later use.\\nLiuqing was able to create a tool which accomplished this and functioned on any tweet collection archived on the cluster in two afternoons' worth of work. He was very satisfied with the functionalities provided by the framework, and estimated that the project may have taken up to a week's worth of work without the framework simplifying the text analysis process for him.\\n\",\n",
      "        \"Chapter 6 Discussion\": \"Chapter 6 Discussion\\nThis chapter will touch on a few main points that should be addressed. Thus, it preemptively answers some common questions that readers may have about the nature of my work.\\nChoice of Tools\\nThe tools provided by this iteration of the framework were designed with two main goals in mind. First is to support the case studies described in Chapter 5. Second is to provide examples of different kinds of tools for future developers to base their work on. For example, LDA was the chosen topic analysis approach because it was natively supported by Spark, was needed by the CS6604 class project case study, and was a complex algorithm that would show off several different functionalities of the framework -all at the same time. Another topic analysis approach could have been implemented just as easily, but LDA met the needs of the project at the time. I would encourage future developers to explore other topic analysis algorithms and write more tools to run the algorithms on TweetCollection data structures.\\nSupported Data Formats\\nSimilar to the choice of tool implementations, the supported data formats were chosen to meet the needs of development happening at the time. Directly supporting HBase, SV files, and Avro files enables a wide variety of data types to be processed into data structures with very little work. Supporting their creation from Scala sequence collections and pre-made RDD structures will enable any data format to be turned into a TweetCollection with only a small amount of extra work from the developer. It would be impractical to attempt to support every single kind of data source, but the provided functionalities encompass a large majority of the kinds of work being done right now. I would encourage future developers to develop new Tweet data structures as the need arises, rather than manually parsing the data and forcing it to conform to already-provided implementations.\\nTweetCollection Functionality\\nAs in the case of the two previous sections, TweetCollection functionality was also motivated by the needs of current projects. In addition, however, it was designed to be easily extended and to support future tweet data structure representations with minimal effort. Hence it was decided to use one data structure with a generic type parameter rather than implement several different kinds of collections. This is also why the TweetCollection supports the mapping of arbitrary functions across the collection. This functionality, in combination with the provided filtering functions which are designed to work with all tweets and give powerful functionality \\\"out of the box,\\\" makes the data structure flexible enough to work with any tweet data structures future developers might create.\\nLessons Learned\\nThroughout the project, there were several occasions where I would start exploring a new problem space and realize that while the framework may have been able to handle the relevant types of problems, it would have been in a clunky or inefficient way. There were multiple occasions where the entire framework had to be refactored to better accommodate a problem that should have been anticipated. The main lesson here is that I should have put more time upfront into explicitly identifying and planning for the problems the framework is trying to address. The other common issue was that of the scope of the framework. Since the intention of the framework is to help researchers solve problems, it was very tempting to spend lots of time implementing tools and functionalities for every project I knew of. Realistically, however, there needed to be a defined scope of what could be developed now and what needed to be left for future work. As the project went on, this scope eventually evolved into directly supporting the four case studies described in Chapter 5 and providing documentation for those who would like to extend the framework in the future to support other problem spaces. A well-defined scope early on would have helped the project be more focused and increased my productivity.\\n\",\n",
      "        \"Chapter 7 Conclusions\": \"Chapter 7 Conclusions\\nConclusion\\nThis project addresses some issues which the DLRL has been facing for some time, even if they have never been seen as major issues. In the past, developers have used various methods to interface with our tweet archive, and to apply various different techniques to represent the data contained within the archive. This approach has been functional, but not optimal. Different developers doing similar things in different ways can lead to complications when other developers need to study or apply others' work to theirs. In this thesis, I have presented a new approach which, based on the case studies presented in Chapter 5, will help development run faster and more smoothly, facilitate collaboration between research projects, and enable developers to create their work in such a way that would make it easy for other developers to reuse it in the future. It is my hope that this framework will serve to benefit the future research endeavors of those in the lab, and that it will be expanded upon by those who have needs which extend beyond its current functionalities.\\nFuture Work\\nThis project was designed with future work in mind, since it is meant to support the research efforts of researchers who want to work with the DLRL tweet digital library. With this in mind, I outline several areas where this project could be expanded in the future.\\nAdditional Framework Tools\\nThe most obvious way this project could be expanded in the future would be to use it to create new tools for researchers to use in their projects. One potential tool would save a tweet collection as a set of metadata and a list of tweet IDs. This kind of export tool would be within Twitter's terms of service while allowing other teams to re-populate the collection by collecting the tweet content for each ID. There could then also be a similar tool which could take an exported collection and \\\"refill\\\" it with content pulled from the Twitter API based on the provided IDs.\\nOther potential tools include more wrapper tools, like the LDAWrapper for Spark's LDA implementation. Spark provides a large set of useful machine learning tools in their MLLib library \\nAnother potential tool would be a more robust cleaning suite that includes functionalities not currently present. One notable functionality that the framework is missing in its current state is the ability to filter profanity words and other explicit content from the collections. There are also some feature extraction tools which could be implemented. For example, named entity recognition could be implemented to identify people, places, or organizations within the collection. A similar system could be implemented to identify things like addresses that are not effectively represented by the way the framework currently handles splitting the tweet text into tokens.\\nAdditional Tweet Implementations\\nThe next area which could be expanded upon is the set of Tweet data structure implementations. As discussed in Chapter 6, the current set of Tweet implementations was motivated by the needs of the projects the framework was supporting as case studies. I believe they form a solid foundation to help support research endeavors, but there are bound to be data formats which may be used in the future which are not currently supported. One example would be supporting other database technologies like MongoDB or MySQL. While those formats could be accommodated by using the \\\"catch-all\\\" sequence based factory method of TweetCollectionFactory, it would be beneficial for developers to directly support those formats with concrete implementations of the Tweet class.\\nUse in Future Classes and Research Projects\\nAs mentioned in Chapter 2, this project was designed to be used by researchers in the Digital Library Research Laboratory to help with future projects and research efforts. I would encourage future offerings of CS 5604 and CS 6604 to make use of the framework to simplify their work and help the students spend less time learning how to use the tools, and therefore more time focusing on learning the concepts and addressing the problems presented in the class.\\n\"\n",
      "    },\n",
      "    \"id\": 17292,\n",
      "    \"title\": \"A Framework for Hadoop Based Digital Libraries of Tweets Masters of Science A Framework for Hadoop Based Digital Libraries of Tweets\",\n",
      "    \"type\": \"thesis\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "docPath = \"/mnt/6t/VT_ETDs/golden_standards/team16/processed/gorbid_fulltext/theses/17292/Bock_M_T_2017.tei.xml\"\n",
    "print(json.dumps(extract_ch_json(docPath,\"thesis\",17292), indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process documents\n",
    "pathlist = Path(\"/mnt/6t/VT_ETDs/golden_standards/team16/processed/gorbid_fulltext/theses/\").glob('*')\n",
    "extractedDoc = {}\n",
    "for path in pathlist:\n",
    "    dIn = str(path)\n",
    "    docId = dIn.split(\"/\")[-1]\n",
    "    #print(docId)\n",
    "    for subPath in path.glob('*'):\n",
    "        if subPath.suffix == '.xml':\n",
    "            docJson = extract_ch_json(subPath,\"thesis\",docId)\n",
    "            if docJson != 0:\n",
    "                with open('/mnt/6t/VT_ETDs/golden_standards/team16/processed/gorbid_fulltext/extracted/thesis/'+docId+'.json', 'w') as outfile:\n",
    "                    json.dump(docJson, outfile)\n",
    "            #print(subPath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test produced result \n",
    "test_json_data = open('/mnt/6t/VT_ETDs/golden_standards/team16/processed/gorbid_fulltext/extracted/thesis/17336.json').read()\n",
    "test_data = json.loads(test_json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
